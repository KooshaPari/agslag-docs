# Report: LLM Decontaminator - Robust Detection of Data Contamination in LLMs (LMSYS)

**Timestamp:** 2025-04-07 17:49:30 PDT

---

## Summary

This work introduces **LLM Decontaminator**, a method to detect **data contamination** in LLM training datasets, especially **paraphrased or translated test samples**.

### Motivation

- Contamination (test data in training) **inflates benchmark scores**.
- Existing detection methods:
  - **n-gram overlap**
  - **embedding similarity**
- These **fail to detect** contamination when test data is **paraphrased or translated**.

### Key Findings

- A 13B model can "beat" GPT-4 on benchmarks by exploiting contamination via rephrasing.
- Contamination is **widespread** in real datasets (Stack, RedPajama).
- Also present in **synthetic data** generated by LLMs (e.g., CodeAlpaca).

### Method: LLM Decontaminator

- Uses LLMs (e.g., GPT-4) to detect **semantic similarity** beyond surface forms.
- More robust to paraphrasing and translation.
- Outperforms existing methods in detecting contamination.

---

## Relevance to Our Project

- Ensures **valid benchmark evaluation** of LLMs and agents.
- Helps **clean training data** to avoid contamination.
- Can be integrated into data pipelines for **robust contamination detection**.
- Important for **fair comparison** and **trustworthy evaluation** of multi-agent systems and LLM orchestration.

---

## Next Steps

- Integrate LLM Decontaminator into data processing workflows.
- Audit existing datasets for contamination.
- Use for **benchmark validation** and **data cleaning**.

---
